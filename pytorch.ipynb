{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is MPS even available? macOS 12.3+\n",
    "print(torch.backends.mps.is_available())\n",
    "\n",
    "# Was the current version of PyTorch built with MPS activated?\n",
    "print(torch.backends.mps.is_built())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(lst)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#Load from NumPy array\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m np_array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray(array)\n\u001b[1;32m      7\u001b[0m np_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np_array)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#Load from list\n",
    "lst = [[1,2,3], [4,5,6]]\n",
    "tensor = torch.tensor(lst)\n",
    "\n",
    "#Load from NumPy array\n",
    "np_array = np.array(array)\n",
    "np_tensor = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1719,  0.4617]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_tensor = torch.tensor([[0.3471, 0.4547, -0.2356]])\n",
    "\n",
    "\n",
    "linear_layer = nn.Linear(in_features=3, out_features=2)\n",
    "\n",
    "\n",
    "output = linear_layer(input_tensor)\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.5314, -0.2070, -0.1172],\n",
       "        [ 0.3880,  0.0824,  0.5148]], requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.0791, 0.4109], requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create network with three linear layers\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10,18),\n",
    "    nn.Linear(18,20),\n",
    "    nn.Linear(20,5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0014,  0.4038,  1.0305,  0.0752,  0.7489, -0.3968,  0.0113, -1.3844,\n",
      "          0.0870, -0.9743]])\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.tensor([[-0.0014, 0.4038, 1.0305, 0.07521, 0.7489, -0.3968, 0.0113, -1.3844, 0.08705, -0.9743]])\n",
    "print(input_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0797, -0.4895,  0.0066,  0.2236, -0.1330]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Pass input_tensor to model to obtain output\n",
    "output_tensor = model(input_tensor)\n",
    "print(output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.tensor([6.0])\n",
    "sigmoid = nn.Sigmoid()\n",
    "output = sigmoid(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9975])\n"
     ]
    }
   ],
   "source": [
    " \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(6,4), #First linear layer\n",
    "    nn.Linear(4,1), #Second linear layer\n",
    "    nn.Sigmoid() #Sigmoid activation function\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1392, 0.8420, 0.0188]])\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.tensor([[4.3, 6.1, 2.3]])\n",
    "\n",
    "#Apply softmax along the last dimension\n",
    "probabilities = nn.Softmax(dim=-1)\n",
    "output_tensor = probabilities(input_tensor)\n",
    "\n",
    "print(output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_data = torch.tensor([[-0.4421, 1.5207, 2.0607, -0.3647, 0.4691, 0.0946],\n",
    "                           [-0.9155, -0.0475, -1.3645, 0.6336, -1.9520, -0.3398],\n",
    "                           [0.7406, 1.6763, -0.8511, 0.2432, 0.1123, -0.0633], \n",
    "                           [-1.6630, -0.0718, -0.1285, 0.5396, -0.0288, -0.8622],\n",
    "                           [-0.7413, 1.7920, -0.0883, -0.6685, 0.4745, -0.4245]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(6,4),\n",
    "    nn.Linear(4,1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "#Pass input data through model \n",
    "output = model(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4668],\n",
      "        [0.3995],\n",
      "        [0.4650],\n",
      "        [0.3746],\n",
      "        [0.4578]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_classes = 3\n",
    "#Create multiclass classification model\n",
    "model=nn.Sequential(\n",
    "    nn.Linear(6,4),\n",
    "    nn.Linear(4, n_classes),\n",
    "    nn.Softmax(dim=-1)\n",
    ")\n",
    "output = model(input_data)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1692],\n",
      "        [-0.1376],\n",
      "        [-0.2210],\n",
      "        [ 0.1542],\n",
      "        [-0.0833]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Regression: forward pass\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(6,4),\n",
    "    nn.Linear(4,1)\n",
    ")\n",
    "\n",
    "output = model(input_data)\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Update network below to perform a multi-class classification with four labels\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(11, 20),\n",
    "  nn.Linear(20, 12),\n",
    "  nn.Linear(12, 6),\n",
    "  nn.Linear(6, 4), \n",
    "  nn.Softmax(dim=-1)\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "F.one_hot(torch.tensor(0), num_classes = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor(1), num_classes = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor(2), num_classes = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8131, dtype=torch.float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "scores = torch.tensor([[-0.1211, 0.1059]])\n",
    "one_hot_target = torch.tensor([[1,0]])\n",
    "criterion = CrossEntropyLoss()\n",
    "criterion(scores.double(), one_hot_target.double())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 1\n",
    "num_classes = 3\n",
    "\n",
    "one_hot_numpy = np.array([0, 1, 0])\n",
    "\n",
    "one_hot_pytorch = F.one_hot(torch.tensor(y), num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "y = [2]\n",
    "scores = torch.tensor([[0.1, 6.0, -2.0, 3.2]])\n",
    "\n",
    "# Create a one-hot encoded vector of the label y\n",
    "one_hot_label = F.one_hot(torch.tensor(y), num_classes=scores.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The first argument of F.one_hot() should be the ground truth label (in this case y), which should be converted to a tensor (e.g. using torch.tensor()) if it is not a tensor data type.\n",
    "The second argument of F.one_hot() is num_classes, which is the number of features in the scores tensor and can be found by accessing the second dimension of the shape of scores (recall that .shape is zero-indexed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "y = [2]\n",
    "scores = torch.tensor([[0.1, 6.0, -2.0, 3.2]])\n",
    "\n",
    "# Create a one-hot encoded vector of the label y\n",
    "one_hot_label = F.one_hot(torch.tensor(y), scores.shape[1])\n",
    "\n",
    "# Create the cross entropy loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Calculate the cross entropy loss\n",
    "loss = criterion(scores.double(), one_hot_label.double())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = nn.Sequential(nn.Linear(16,8)\n",
    "                      nn.Linear(8,4),\n",
    "                      nn.Linear(4,2))\n",
    "prediction = model(sample)\n",
    "\n",
    "#Calculate the loss and compute the gradients\n",
    "criterion = CrossEntropyLoss()\n",
    "loss = criterion(prediction, target)\n",
    "loss.Backward()\n",
    "\n",
    "#Access each layer's gradients\n",
    "model[0].weight.grad, model[0].bias.grad\n",
    "model[1].weight.grad, model[0].bias.grad\n",
    "model[2].weight.grad, model[0].bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr = 0.001\n",
    "\n",
    "#Update the weights\n",
    "weight = model[0].weight\n",
    "weight_grad = model[0].weight.grad\n",
    "weight = weight - lr * weight_grad\n",
    "\n",
    "#Update the biases\n",
    "bias = model[0].bias\n",
    "bias_grad = model[0].bias.grad\n",
    "bias = bias - lr * bias_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In previous exercises, you used linear layers to build networks.\n",
    "\n",
    "Recall that the operation performed by nn.Linear() is to take an input \n",
    " and apply the transformation \n",
    " ,where \n",
    " and \n",
    " are two tensors (called the weight and bias).\n",
    "\n",
    "A critical part of training PyTorch models is to calculate gradients of the weight and bias tensors with respect to a loss function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "loss = criterion(preds, target)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(weight.grad)\n",
    "print(bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(16, 8),\n",
    "                      nn.Sigmoid(),\n",
    "                      nn.Linear(8, 2))\n",
    "\n",
    "weight_0 = model[0].weight\n",
    "\n",
    "bias_1 = model[2].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "loss = criterion(pred, target)\n",
    "loss.backward()\n",
    "\n",
    "# Update the model's parameters using the optimizer\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mean_squared_loss(prediction, target):\n",
    "    return np.mean((prediction - target)**2)\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(prediction, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = TensorDataset(torch.tensor(features).float(), torch.tensor(target).float())\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "model = nn.Sequential(nn.Linear(4,2),\n",
    "                      nn.Linear(2,1))\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        #Set the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        #Get feature and target from the data loader\n",
    "        feature, target = data\n",
    "        #Run a forward pass\n",
    "        pred = model(feature)\n",
    "        #Compute loss and gradients\n",
    "        loss = criterion(pred, target)\n",
    "        loss.backward()\n",
    "        #Update the parameters\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = np.array(10)\n",
    "y = np.array(1)\n",
    "\n",
    "# Calculate the MSELoss using NumPy\n",
    "mse_numpy = np.mean((y_hat - y)**2)\n",
    "\n",
    "# Create the MSELoss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Calculate the MSELoss using the created loss function\n",
    "mse_pytorch = criterion(torch.tensor(y_hat).float(), torch.tensor(y).float())\n",
    "print(mse_pytorch)\n",
    "\n",
    "# Loop over the number of epochs and the dataloader\n",
    "for i in range(num_epochs):\n",
    "  for data in dataloader:\n",
    "    # Set the gradients to zero\n",
    "    optimizer.zero_grad()  \n",
    "    # Run a forward pass\n",
    "    feature, target = data\n",
    "    prediction = model(feature)    \n",
    "    # Calculate the loss\n",
    "    loss = criterion(prediction, target)    \n",
    "    # Compute the gradients\n",
    "    loss.backward()\n",
    "    # Update the model's parameters\n",
    "    optimizer.step()\n",
    "show_results(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "relu = nn.ReLU()\n",
    "\n",
    "\n",
    "leaky_relu = nn.LeakyReLU(negative_slope=0.05)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ReLU function with PyTorch\n",
    "relu_pytorch = nn.ReLU()\n",
    "\n",
    "# Apply your ReLU function on x, and calculate gradients\n",
    "x = torch.tensor(-1.0, requires_grad=True)\n",
    "y = relu_pytorch(x)\n",
    "y.backward()\n",
    "\n",
    "# Print the gradient of the ReLU function for x\n",
    "gradient = x.grad\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a leaky relu function in PyTorch\n",
    "leaky_relu_pytorch = nn.LeakyReLU(negative_slope=0.05)\n",
    "\n",
    "x = torch.tensor(-2.0)\n",
    "# Call the above function on the tensor x\n",
    "output = leaky_relu_pytorch(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layers are made of neurons\n",
    "# - Linear layers are fully connected\n",
    "# - Each neuron of a layer connected to each neuron of previous layer\n",
    "# - A neuron of a linear layer: - computes a linear operation using all neurons of previous layer \n",
    "#                               - contains N+1 learnable parameters\n",
    "#                               - where N = dimesion of previous layer's outputs\n",
    "\n",
    "# Tewaking the number of hidden layers\n",
    "# Input and output layers dimensions are fixed - input layer depends on the number of features n_features\n",
    "#                                              - output layer depends on the number of categories n_classes\n",
    "# We can use as many hidden layers as we want\n",
    "# Increasing the number of hidden layers = increasing the number of parameters = increasing the model capacity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting the number of parameters \n",
    "\n",
    "model = nn.Sequential(nn.Linear(8,4),\n",
    "                      nn.Linear(4,2))\n",
    "#Manually calculating the number of parameters:\n",
    "# - first layer has 4 neurons, each neuron has 8+1 parameters = 36 parameters\n",
    "# - second layer has 2 neurons, each neuron has 4 + 1 parameters = 10 parameters\n",
    "# total = 46 learnable parameters\n",
    "\n",
    "# .numel(): returns the number of elements in the tensor\n",
    "\n",
    "total = 0\n",
    "for parameter in model.parameters()\n",
    "    total += parameter.numel()\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 8\n",
    "n_classes = 2\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Create a neural network with less than 120 parameters\n",
    "model = nn.Sequential(nn.Linear(n_features, 8),\n",
    "                      nn.Linear(8, 4),\n",
    "                      nn.Linear(4, n_classes))\n",
    "output = model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 8\n",
    "n_classes = 2\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Create a neural network with more than 120 parameters\n",
    "model = nn.Sequential(nn.Linear(n_features, 16),\n",
    "                      nn.Linear(16, 8),\n",
    "                      nn.Linear(8, 3), \n",
    "                      nn.Linear(3, n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utilis.data import TensorDataset\n",
    "from torch.utilis.data import DataLoader\n",
    "pd.read_csv('animals.csv')\n",
    "#Define input features\n",
    "features = animals.iloc[:, 1:-1]\n",
    "X = features.to_numpy()\n",
    "print(X)\n",
    "\n",
    "#Define target features (ground truth)\n",
    "\n",
    "target = animals.iloc[:, -1]\n",
    "y = target.to_numpy()\n",
    "\n",
    "#Instatiate dataset class\n",
    "dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float())\n",
    "\n",
    "#Access an individual sample\n",
    "sample = dataset[0]\n",
    "input_sample, label_sample = sample\n",
    "print('input sample:', input_sample)\n",
    "print('label_sample:', label_sample)\n",
    "\n",
    "\n",
    "batch_size = 2 # Kiedy bedziemy iterowac po module ladujacym dane, batch_size bedzie okreslac ile probek pobieramy ze zbioru w jednej iteracji.\n",
    "               # Parametr shuffle informuje moduł ładujący dane o tasowaniu danych w kazdej iteracji.\n",
    "shuffle = True\n",
    "\n",
    "#Create a DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "#Iterate over the dataloader\n",
    "for batch_inputs, batch_labels in dataloader:\n",
    "    print('batch inputs', batch_inputs)\n",
    "    print('batch labels', batch_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "training_loss = 0.0\n",
    "for i, data in enumerate(trainloader, 0):\n",
    "    # Run the forward pass\n",
    "    ...\n",
    "    # Calculate the loss\n",
    "    loss = criterion(outputs, labels)\n",
    "    # Calculate the gradients\n",
    "    ...\n",
    "    # Calculate and sum the loss\n",
    "    training_loss += loss.item()\n",
    "epoch_loss = training_loss / len(trainloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating validation loss\n",
    "#After the training epoch, we iterate over the validation set and calculate the average validation loss\n",
    "validation_loss = 0.0\n",
    "model.eval() # Put model in evaluation mode\n",
    "with torch.no_grad(): #Speed up the forward pass\n",
    "    for i, data in enumerate(validationloader, 0):\n",
    "        #Run the forward pass\n",
    "        ...\n",
    "        #Calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        validation_loss += loss.item()\n",
    "epoch_loss = validation_loss / len(validationloader)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating accuracy with torchmetrics\n",
    "import torchmetrics\n",
    "#Create accuracy metric using torch metrics\n",
    "metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=3)\n",
    "for i, data in enumerate(dataloader, 0):\n",
    "    features, labels = data\n",
    "    outputs = model(features)\n",
    "    # Calculate accuracy over the batch\n",
    "    acc = metric(output, labels.argmax(dim=-1))\n",
    "\n",
    "# Calculate accuracy over the whole epoch\n",
    "acc = metric.compute()\n",
    "print(f\"Accuracy on all data: {acc}\")\n",
    "#Reset the metric for the next epoch (training or validation)\n",
    "metric.reset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = nn.Sequential(nn.Linear(8,4),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout(p=0.5)) # tutaj ustawiamy 50% tensora wyjsciowego na zero\n",
    "\n",
    "features = torch.randn((1,8))\n",
    "model(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regularization with weight decay\n",
    "optimizer = optim.SGD(model.parameters(), lr = 1e-3, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Improving model performance\n",
    "\n",
    "#fine-tune hyperparameters\n",
    "#Grid search\n",
    "for factor in range(2,6):\n",
    "    lr = 10 ** -factor\n",
    "#Random search\n",
    "factor = np.random.uniform(2,6)\n",
    "lr = 10 ** -factor\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "images",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
